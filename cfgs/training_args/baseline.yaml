# args for Trainer
# see https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments
output_dir: ./ckpt/{dataset}/baseline_{seed}
do_train: true
do_eval: true
eval_strategy: "epoch"
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
gradient_accumulation_steps: 1
learning_rate: 5e-5
weight_decay: 0.01
lr_scheduler_type: "constant"
num_train_epochs: 200
logging_dir: ./logs/{dataset}/baseline_{seed}
logging_strategy: 'epoch'
#logging_steps: 100
save_strategy: "epoch"
save_total_limit: 3
seed: 22
fp16: true
#fp16_full_eval: true
dataloader_num_workers: 6
load_best_model_at_end: true
metric_for_best_model: f1
greater_is_better: true
report_to: wandb
run_name: "baseline_{dataset}_{seed}"
dataloader_persistent_workers: true
torch_compile: true
predict_with_generate: true
generation_max_length: 64
generation_num_beams: 4