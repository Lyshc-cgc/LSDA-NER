# Attention! ALL paths must be relative to the 'run.py' file!

dataset_name: conll2003
file_path: data/conll2003/conll2003.py
num_proc: 6  # the number of processes to preprocess the data
nested: False  # whether the entities are nested

# process config
tokens_field: tokens  # the field name of the tokens, please make sure the field name is consistent with the data
ner_tags_field: ner_tags  # the field name of the NER tag, please make sure the field name is consistent with the data
data_batch_size: 4096 # batch size for data processing
shuffle: True  # whether to shuffle the data
preprocessed_dir: data/conll2003/preprocess  # the directory to store the preprocessed data
continue: True  # whether to continue to use the data last time
continue_dir: data/conll2003/continue  # the directory to store the continued data to be annotated

# few-shot settings
support_set: True  # whether to sample support set
k_shot:  # the number of examples for each label in the training set
  - 20
  - 10
  - 5
#  - 1
sample_split: train  # the dataset split you want to sample support set from.
ss_cache_dir: data/conll2003/support_set  # the directory to store the support set cache

labels:
  PER: person
  ORG: organization
  LOC: location
  MISC: miscellaneous

raw_bio: True  # a flag to indicate whether the labels are in BIO format in the raw dataset.(data/*/raw/*)
raw_label2id:  # the label2id mapping in the raw datasets. (data/*/raw/*)
  O: 0
  B-PER: 1
  I-PER: 2
  B-ORG: 3
  I-ORG: 4
  B-LOC: 5
  I-LOC: 6
  B-MISC: 7
  I-MISC: 8